---
title: 
author: 
date: "`r Sys.Date()`"
output:
  pdf_document: 
    fig_width: 10
    fig_height: 5
    keep_tex: yes
    latex_engine: lualatex
    #citation_package: biblatex
  html_document: default
bibliography: reference.bib
link-citations: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Clearing environment , loading appropriate libraries and reading datasets

```{r 1a. Loading Library, message=FALSE, warning=FALSE}
rm(list=ls())

library("readxl")
library("ggplot2")
library("tidyverse")
library("fpp2")
library(tseries)
library(forecast)

library(vtable)
library(tseries)
library(car)
library(seasonal)

options(scipen = 999)  ## this function avoids the use of 'e+...." in the p-values.
set.seed(123) ## to ensure randomly generated data remains the same
#par(mar = c(5, 4, 4, 2) + 0.1) # set plot area

```

```{r Setting Working Directory,  message=FALSE}
setwd("")

#getwd()
```

```{r 1b. Loading Data}

data_9320 <- read_excel("CPI_ch.xls", col_names = TRUE)
data_0012 <- read_excel("CPI_ch_res.xls", col_names = TRUE)

names(data_9320)

```

### 1b. Exploring Data:

```{r  1c. Summary Statistics }
st(data_9320, title = "Summary Statistics_1993Q1_2020Q3",
   labels = "cpi_9320", out = "csv" , file='Tables/1993Q1_2020Q4_summary')
st(data_0012,  title = "Summary Statistics_2000Q1_2012Q4",
   labels = "cpi_0012",out = "csv" ,  file='Tables/2000Q1_2012Q4_summary')

```

cpi_9320: with a mean value of `r mean(data_9320$cpi)`, which suggests that the average index has increased over time relative to the base year and with a standard deviation of `r sd(data_9320$cpi)`, indicates that the values of the index are relatively spread out from the mean.The wide range suggests that the data has a relatively large variability, which may indicate that there is a greater diversity of values or a higher degree of variability in the underlying phenomenon being measured.

cpi_0012: the values in the dataset tend to cluster around the mean value of `r mean(data_0012$cpi)` indicating that there is some variability in the data, with most of the values falling within approximately one standard deviation of the mean. Specifically, about 68% of the data values should be within the range of 70.8 to 87.2 (mean Â± 1 standard deviation). The remaining 32% of the data values may fall outside this range, with some being more extreme than others.

## 2: Creating and Plotting time series data with tsplot:

```{r, 2a. Creating Time Series}
# time series from 1993Q1 to 2020Q4
ts_data9320 <- ts(data_9320$cpi, start=c(1993,1), frequency = 4) 

 # time series from 2000Q1 to 2012Q4
ts_data0012 <- ts(data_0012$cpi, start=c(2000,1), frequency = 4)

```

```{r 2b. 1993Q1 to 2020Q4ts, fig.path='Plots/'}
# plotting time series objects for 1993Q1 to 2020Q3 (ts_data9320)
ts.plot(ts_data9320, main = "CPI over Time (1993Q1 to 2020Q3 ts)", xlab = "Year", ylab = "cpi")
```

**Evaluating series Plot**:

1.  The plot demonstrates non-linearity, with trend and possibly some seasonality.
2.  The series has a non-zero mean.
3.  It also appears to be an exponential series.
4.  There is no noticeable break or presence of an outlier

According to [@hyndman_2018_forecasting] ,a time series with a trend and/or seasonality is typically nonstationary suggesting that *1993Q1 to 2020Q4 time series is unquestionably nonstationary*

However, evaluation the residuals of a linear regression and ADF test result can assist to further detect trend, seasonality and outliers in the data.

### 2c.Evaluating trend using a regression model 

```{r 2c. estimating trend and seasonality , fig.path='Plots/'}
fit.cpi <- tslm(ts_data9320 ~ trend + season)

summary(fit.cpi)
```

-   The trend coefficient is significant at the 1%-level, indicating that the time series has a trend component
-   The seasonal dummy coefficients are statistically significant ( p-values \> 0.05), suggesting that there is seasonality in the data.
-   The coefficient estimates for these variables are negative, which suggests that the data has a decreasing pattern during the corresponding seasons.
-   The adjusted R-squared value of `r summary(fit.cpi)$r.squared` indicates that the model fits the data well, and the F-statistic of `r summary(fit.cpi)$fstatistic[1]`  with a very small p-value `r summary(fit.cpi)$p.value` (\<0.001) suggests that the model is significant.

*based on the regression model output, it appears that the data has both trend and seasonality.*

### 2d. Fitted Vs Actual Data.

```{r 2d. Fitted Vs Actual, fig.path='Plots/'}
autoplot(ts_data9320, series="Actual cpi Time Series ") +
  autolayer(fitted(fit.cpi), series="Fitted Values") +
  xlab("Quarter") + ylab("cpi") +
  ggtitle("Quarterly cpi Time Series") +   
  theme(legend.position = "right", legend.justification = "right")

```

-   The fitted data shows a very clear seasonal pattern, which the actual data does not always match. Perhaps this is due to irregular or random variations despite seasonal patterns.
-   It also seems like the variability of the seasonal effect increases over time.
-   check regression residuals and DW test to validate trend and seasonality

```{r 2e. tslm residuals , fig.path='Plots/'}
checkresiduals(fit.cpi)

durbinWatsonTest(fit.cpi)
```

-   ACF shows a clear Serial correlation in data with a BG test p-value \< 0.05(significant level).
-   p-value of 0 for Durbin-Watson statistic also indicates that there is autocorrelation present in the residuals..

***Basically 1993Q1 to 2020Q4 time series data is non-stationary with some autocorrelation***

## 3: Log Transformation:1993Q1 to 2020Q3 (ts_data9320)

```{r 3a.Log Variable and log Time Series}

# Taking the natural logarithm (log base e) of the CPI variable
data_9320$logCPI <- log(data_9320$cpi)

# Generating a time series object from log CPI variable
ts_logCPI9320 <- ts(data_9320$logCPI, start = c(1993, 1), frequency = 4)

# Summary
table3 <- summary(ts_logCPI9320,  title = "Summary Statistics_logCPI",
                  labels = "logcpi_9320",out = "csv" ,
                  file='Tables/logCPI_summary' )

```



```{r 3b. CPI Vs logCPI , fig.path='Plots/'}
# Set up the plotting window
par(mfrow = c(1, 2))

# Plot the time series on the left
plot(ts_data9320, main = "Time Series" , xlab = "Year", ylab = "CPI")

# Plot the log of the time series on the right
plot(ts_logCPI9320, main = "Log of Time Series" , xlab = "Year", ylab = "logCPI")
```
*Although no distinct difference exist between both plots, the log transformed series however, has helped*

According to [@west_2021_best], the use of log transformation is considered best practice in statistics because:
1.  it normalizes the distribution,making the data easier to work with and more amenable to statistical modeling
2.  it converts the scale to an additive scale which can make it easier to model using linear regression techniques
3.  it linearize relationship between variables making it easier to model using linear regression or other techniques that assume a linear relationship.
4.  it stabilizes the variance of the series

Log transformation also help normalize data that is highly skewed or has a long-tailed distribution

### 3b. Re-fitting log transformed series using LM

```{r 3c. logCPI_LM, fig.path='Plots/'}
# Create a new data frame with the log transformed CPI and the year variable
dflog <- data.frame(ts_logCPI9320, year = as.numeric(time(ts_logCPI9320)))

# Fit a linear regression model
fit_log <- tslm(ts_logCPI9320 ~ year , data = dflog)

# Visualize the linear regression model
plot(dflog$year, dflog$ts_logCPI9320, type = "l", col = "black", xlab =
       "Year", ylab = "Log CPI")
abline(fit_log, col = "red") # add a linear regression line
legend("bottomright", legend = c("Log CPI", "Linear Regression"), 
       lty = c(1, 1), col = c("black", "red")) # add a legend

```

```{r 3d. Log LM evaluation, fig.path='Plots/'}
#check residuals
checkresiduals(fit_log)
```

## 4: Classical decomposition of both the CPI and the log CPI series: 1993Q1 to 2020Q3 (ts_data9320)

[@hyndman_2018_chapter] states that in the case of a CPI series, seasonal variations are often proportional to the level of the index, suggesting that a multiplicative decomposition may be more appropriate. This is because changes in prices can have a magnifying effect on seasonal variations.

Likewise,a log-multiplicative decomposition may be more appropriate since the seasonal fluctuations are proportional to the level of the series with a varying degree of variability over time.

*Also, additive struggles with seasonal components a bit more and contributes more of the data patterns to irregular variation*

However, using x11 decompostion is more appropriate since the seasonal fluctuations in the data vary in proportion to the trend of the data. it uses a multiplicative decomposition method to remove the seasonal fluctuations in the data, while retaining the underlying trend and cyclical components. The resulting seasonally adjusted series can then be used to analyze the underlying trend and cyclical behavior of the time series. also, it is widely used in the field of economics and finance, as it provides a powerful tool for analyzing the seasonal behavior of economic and financial time series data.

### 4a. Classical Multiplicative decomposition of CPI series

```{r 4a. CPI Decomposition , fig.path='Plots/'}
ts_data9320 %>% decompose(type="multiplicative") %>%
autoplot() + xlab("Quarter") +
ggtitle("Multiplicative Decomposition of CPI Time Series")

```

### 4b. Classical Multiplicative decomposition of log CPI series

```{r 4b. logCPI Decomposition , fig.path='Plots/'}
ts_logCPI9320 %>% decompose(type="multiplicative") %>%
autoplot() + xlab("Quarter") +
ggtitle("Multiplicative Decomposition oflog CPI Time Series")
```

### 4c. X11 Decomposition Analysis

```{r 4c. X11 Decomposition , fig.path='Plots/'}
decomp <- ts_data9320 %>% seas(x11="")
autoplot(decomp) +
ggtitle("X11 decomposition of Quarterly CPI Time Series")
```

```{r 4d. X11 Analysis}
decomp <-decompose(ts_data9320, "multiplicative")

# Extracting all components 
trend <- decomp$trend 
seasonal <- decomp$seasonal 
irregular  <- decomp$random 

# Compute summary statistics
cat(" trend component: average value of ", mean(trend, na.rm = TRUE),
    "and a variance of: ", var(trend, na.rm = TRUE), "\n")

cat( " seasonal component: mean of ", mean(seasonal, na.rm = TRUE), 
     "and a variance of " , var(seasonal, na.rm = TRUE),"\n")

cat(" irregular component: mean of ", mean(irregular, na.rm = TRUE), 
    " and a variance of",  var(irregular, na.rm = TRUE),"\n")

```

The trend of the time series has an average value of ,`r mean(trend, na.rm = TRUE)`, and a variance of:, `r var(trend, na.rm = TRUE)`,an indication that there is a significant  amount of fluctuation in the trend component over time and it may be difficult to make  accurate long-term predictions based on this component due to its volatility.

The mean of the seasonal component `r mean(seasonal, na.rm = TRUE)`,is very close to 
1,indicating that the seasonal pattern is relatively consistent across time and with a 
variance of `r var(seasonal, na.rm = TRUE)`, the seasonal pattern does not vary much from the mean value allowing for accurate short-term predictions.

With an average of ,`r mean(irregular, na.rm = TRUE)`,and a low variance of ,
`r var(irregular, na.rm = TRUE)`, for the irregular component, the time series is relatively stable and predictable.


## 5. Timeseries: 2000Q1 to 2012Q4 (ts_data0012)

### 5a. Data Summary

```{r Summary}
cpi0012 = ts_data0012

# Taking the natural logarithm (log base e) of the CPI variable
data_0012$logCPI <- log(data_0012$cpi)
# Generating a time series object from log CPI variable
logcpi0012 <- ts(data_0012$logCPI, start = c(2000, 1), frequency = 4)

cat("Variance of the original series:", var(cpi0012, na.rm = TRUE), "while
    Variance of the log transformed series:",
    var(logcpi0012, na.rm =TRUE),"\n")
```

The original series may have a high degree of variability or volatility, which can make it difficult to analyze or model while the log-transformed series has a much lower variance and may make it easier to identify patterns or trends in the data and to model the series using statistical methods.

overall, it is preferable to use the log-transformed series for differencing, as it can potentially lead to more accurate and reliable analysis and modeling results [@west_2021_best]. It also made our skewed original data more normal. 

### 5b. Stationarity Test

```{r 5b. stationarityTest , fig.path='Plots/'}

adf.test(logcpi0012) # ADF test

Box.test(logcpi0012, lag=1, type="Ljung-Box") # LB test

par(mar = c(5, 4, 4, 2) + 0.1) # set plot area
acf(logcpi0012, lag.max=32) # 
```

-   ADF test shows a p-value of `r adf.test(logcpi0012)$p.value` \> the commonly used significance level of 0.05, the logCPI series can be considered non-stationary.
-   Also, Box-Ljung test shows a p-value of `r Box.test(logcpi0012, lag=1, type="Ljung-Box")$p.value`(\<0.05), which suggests that there is a trend and evidence of autocorrelation in the series as supported by the ACF plot suggesting that a transformation may be necessary to stabilize the variance of the series before fitting an ARIMA model and making forecasts.

***Therefore, based on the result of the ADF test, it can be concluded that the logCPI series is non-stationary and with an evidence of autocorrelation.***

## 6. Differencing, ARIMA & Forecasting : logcpi0012

Series is non-stationary.However, for accurate statistical analysis, differencing the series to further remove trend and seasonality is required.

### 6a. Differencing

```{r  6ai. First Difference, fig.path='Plots/' }

diff_logCPI <- diff(logcpi0012) # first-order difference to remove trend

ts.plot(diff_logCPI) 

```
*Seems like the pattern has died out, but seasonality persists an indication that the series is not stationary.*

```{r  6aii. fourth-order difference , fig.path='Plots/' }
 # fourth-order difference to remove seasonality
diff_logCPI_seasonal <- diff(logcpi0012, differences = 1, lag=4)

ts.plot(diff_logCPI_seasonal) 

```

*series is not yet stationary, and the trend is evident, posing an autocorrelation issue. The Seasonally differentiated series may be further differenced with a lag of 1 to obtain a white noise process*

```{r  6aiii. combined difference , fig.path='Plots/' }
# combined difference to remove trend & seasonality
diff_combined <- diff(diff_logCPI_seasonal , differences = 1, lag=1) 

ts.plot(diff_combined, main = "Differenced Time Series")

```
*Plot looks more like a white noise process and the stationarity and auto-correlation status can be assessed using ADF test and the ACF plot;*

### 6b. ACF Plots/ Seasonal ARIMA (SARIMA) models:

```{r 6bi. ACF , fig.path='Plots/'}
## set up plot region
par(mar = c(5, 4, 4, 2) + 0.1)

ggtsdisplay(diff_logCPI) # plotting first  differenced series with ACF
ggtsdisplay(diff_logCPI_seasonal)  # plotting fourth differenced series with ACF
ggtsdisplay(diff_combined) # plotting last differenced series with ACF

```
ACF plot can be used to identify the term of the moving average (MA) component or the autoregressive component (AP)of a time series [@sangarshanan_2018_time].

#### 6bi. ARIMA 
*first differenced series*: 
+ there is a Positive autocorrelation at lag 1 Hence an AR term will be used.
+ both the ACF plot and PACF plot show a sharp drop-off, suggesting that the time series
may be a white noise process with d = 1:  ARIMA(p,d,q) = ARIMA(1,1,0)

*fourth differenced series:*
+ there is a Positive autocorrelation at lag 1 Hence an AR term will be used
+ the PACF plot shows a sharp drop-off and the ACF plot shows a gradual decay suggesting that the time series has an AR(p) component, a difference D=1, but no MA(q) component. with ARIMA(p,d,q)(P,D,Q)[4] = ARIMA(1,1,0)(1,1,0)[4]

*diff_combined series:*
+ the PACF plot shows a sharp drop-off and the ACF plot shows another gradual decay suggesting that the time series has an AR(p) component and two significant MA(q) component with updated model = ARIMA(2,1,2)(1,1,0)[4]

##### Evaluating initial ARIMA model : ARIMA(2,1,2)(1,1,0)[4]

```{r 6bi. ARIMA1 , fig.path='Plots/'}
fit_arima<- Arima(logcpi0012, order = c(2, 1, 2),
                  seasonal = list(order = c(1, 1, 0), period = 4))

summary(fit_arima)

checkresiduals(fit_arima) #checkresiduals(arima_model)

```

-   The LB test p-value of `r checkresiduals(fit_arima)$p.value`, which is less than the significance level of 0.05 suggests that the residuals does not follow a white noise process with some autocorrelation and the model might not be apppropriate for future forecasting

With a significant Negatively Autocorrelated at Lag â 8, model needs a little transformation, by including a MA(Q) component to remove the seasonal lag : ARIMA(2,1,2)(1,1,1)[4]

##### Evaluating second ARIMA model : ARIMA(2,1,2)(1,1,1)[4]

```{r 6bii. ARIMA2 , fig.path='Plots/'}

fit_arima2<- Arima(logcpi0012, order = c(2, 1, 2), 
                   seasonal = list(order = c(1, 1, 1), period = 4))

summary(fit_arima2)

checkresiduals(fit_arima2) #checkresiduals(arima_model)

```

- The LB test p-value of `r checkresiduals(fit_arima2)$p.value`, which is \> than the significance level of 0.05 suggests that the residuals follows a white noise process with no autocorrelation and the model might be apppropriate for future forecasting.

- This is further confirmed by the relatively low values in the mean error(ME), mean absolute error (MAE) and but the root mean squared error (RMSE) and mean absolute percentage error (MAPE). the mean percentage error (MPE) is close to zero, indicating that the model has no systematic bias.

### 6c. auto.arima

Using auto.arima to confirm if our model is appropriate:

According to [@hyndman_2008_automatic], comparing the performance of the `auto.arima` model to other models using AIC or BIC is essential to determine a better model. a lower value of these criteria indicate better model fit.

```{r 6ci. auto.arima , fig.path='Plots/'}

auto_arima <- auto.arima(logcpi0012) 

checkresiduals(auto_arima)

cat("AIC of auto.arima model :", AIC(auto_arima), " while ", "\n",
    "AIC of manually specified model (fit_arima2):", AIC(fit_arima2),"\n")

```

Since the manually specified model has a lower AIC value, it is considered a better model than the auto.arima model. ***Therefore, fit_arima2 will be used for forecasting.***

### 6cii. Forecasting

```{r 6cii.  Forecasting }
start <- (2013)
end <- c(2020)

forecast:: forecast(fit_arima2, h = (length(start:end)*4)) 

# Save the table to a CSV file
write.table(forecast:: forecast(fit_arima2, h = (length(start:end)*4)),
            file = 'Tables/forecast_table.csv', sep = ",",
            row.names = TRUE)
```

### 6d. Actual Vs Forecast

```{r 6d. Actual Vs Forecast , fig.path='Plots/'}

# plot the forecast result
forecast_values <- forecast(fit_arima2, h = (length(start:end)*4))

plot(forecast_values, main = "logActual(2000Q1 to 2020Q3) Vs
      logForecast(2000Q1 to 2020Q3)" , xlab = "Year", ylab = "logCPI" )
     lines(ts_logCPI9320, col = "red")# add second time series to plot
     legend("topleft", legend = c("Forecast Value", "Actual Values"),
            lty = c(1, 1), col = c("blue", "red")) # add a legend

accuracy(forecast_values, ts_logCPI9320)

```

Based on the evaluation metrics for the test set, the model seems to be a reasonable fit [@hyndman_2008_automatic] withe below reasons: 

+ RMSE value of `r accuracy(forecast_values, ts_logCPI9320)[2, "RMSE"]`  suggests that the  model has an average error of about `r accuracy(forecast_values, ts_logCPI9320)[2,"RMSE"]` units when making predictions on the test set. 

+ MAPE value of `r accuracy(forecast_values, ts_logCPI9320)[2, "MAPE"]` indicates that the  model's percentage error is about `r 100* accuracy(forecast_values, ts_logCPI9320)[2,"RMSE"]`% , on average. 

+ ACF1 value of `r accuracy(forecast_values, ts_logCPI9320)[2, "ACF1"]` indicates that the model has captured the autocorrelation structure of the data well. 

+ However, the Theil's U statistic of `r accuracy(forecast_values, ts_logCPI9320)[2, "Theil's U"]` suggests that the model's performance is only slightly better than a naive forecast. Therefore, while the model is reasonably good, there may be room for improvement.

## References 







